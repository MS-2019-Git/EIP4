# EIP4
Assignment 3:

1) Base network validation accuracy and log:   Accuracy on test data is: 82.79

Epoch 1/50
390/390 [==============================] - 30s 77ms/step - loss: 1.8503 - acc: 0.2845 - val_loss: 1.4858 - val_acc: 0.4423
Epoch 2/50
390/390 [==============================] - 20s 51ms/step - loss: 1.3615 - acc: 0.5065 - val_loss: 1.1583 - val_acc: 0.5838
Epoch 3/50
390/390 [==============================] - 20s 52ms/step - loss: 1.1040 - acc: 0.6059 - val_loss: 1.0074 - val_acc: 0.6464
Epoch 4/50
390/390 [==============================] - 20s 52ms/step - loss: 0.9628 - acc: 0.6623 - val_loss: 0.8764 - val_acc: 0.6974
Epoch 5/50
390/390 [==============================] - 20s 52ms/step - loss: 0.8621 - acc: 0.7027 - val_loss: 0.7920 - val_acc: 0.7272
Epoch 6/50
390/390 [==============================] - 21s 53ms/step - loss: 0.7818 - acc: 0.7323 - val_loss: 0.7529 - val_acc: 0.7435
Epoch 7/50
390/390 [==============================] - 20s 52ms/step - loss: 0.7304 - acc: 0.7504 - val_loss: 0.7291 - val_acc: 0.7522
Epoch 8/50
390/390 [==============================] - 21s 53ms/step - loss: 0.6858 - acc: 0.7669 - val_loss: 0.6739 - val_acc: 0.7741
Epoch 9/50
390/390 [==============================] - 20s 53ms/step - loss: 0.6552 - acc: 0.7788 - val_loss: 0.6456 - val_acc: 0.7779
Epoch 10/50
390/390 [==============================] - 21s 53ms/step - loss: 0.6250 - acc: 0.7855 - val_loss: 0.6331 - val_acc: 0.7888
Epoch 11/50
390/390 [==============================] - 20s 52ms/step - loss: 0.5911 - acc: 0.7981 - val_loss: 0.6459 - val_acc: 0.7872
Epoch 12/50
390/390 [==============================] - 21s 53ms/step - loss: 0.5757 - acc: 0.8050 - val_loss: 0.6613 - val_acc: 0.7795
Epoch 13/50
390/390 [==============================] - 20s 52ms/step - loss: 0.5559 - acc: 0.8138 - val_loss: 0.6182 - val_acc: 0.7936
Epoch 14/50
390/390 [==============================] - 21s 53ms/step - loss: 0.5278 - acc: 0.8200 - val_loss: 0.5916 - val_acc: 0.8025
Epoch 15/50
390/390 [==============================] - 20s 52ms/step - loss: 0.5243 - acc: 0.8236 - val_loss: 0.5869 - val_acc: 0.8024
Epoch 16/50
390/390 [==============================] - 20s 52ms/step - loss: 0.4960 - acc: 0.8311 - val_loss: 0.5957 - val_acc: 0.8051
Epoch 17/50
390/390 [==============================] - 20s 52ms/step - loss: 0.4907 - acc: 0.8336 - val_loss: 0.5845 - val_acc: 0.8085
Epoch 18/50
390/390 [==============================] - 21s 53ms/step - loss: 0.4833 - acc: 0.8344 - val_loss: 0.5864 - val_acc: 0.8067
Epoch 19/50
390/390 [==============================] - 21s 53ms/step - loss: 0.4687 - acc: 0.8407 - val_loss: 0.6147 - val_acc: 0.7995
Epoch 20/50
390/390 [==============================] - 21s 53ms/step - loss: 0.4640 - acc: 0.8427 - val_loss: 0.5815 - val_acc: 0.8113
Epoch 21/50
390/390 [==============================] - 21s 55ms/step - loss: 0.4449 - acc: 0.8484 - val_loss: 0.5823 - val_acc: 0.8090
Epoch 22/50
390/390 [==============================] - 21s 54ms/step - loss: 0.4370 - acc: 0.8510 - val_loss: 0.5872 - val_acc: 0.8155
Epoch 23/50
390/390 [==============================] - 21s 54ms/step - loss: 0.4266 - acc: 0.8547 - val_loss: 0.5806 - val_acc: 0.8156
Epoch 24/50
390/390 [==============================] - 21s 54ms/step - loss: 0.4194 - acc: 0.8571 - val_loss: 0.5944 - val_acc: 0.8140
Epoch 25/50
390/390 [==============================] - 21s 54ms/step - loss: 0.4114 - acc: 0.8607 - val_loss: 0.5729 - val_acc: 0.8136
Epoch 26/50
390/390 [==============================] - 21s 54ms/step - loss: 0.4042 - acc: 0.8625 - val_loss: 0.5735 - val_acc: 0.8171
Epoch 27/50
390/390 [==============================] - 21s 54ms/step - loss: 0.3980 - acc: 0.8644 - val_loss: 0.5891 - val_acc: 0.8161
Epoch 28/50
390/390 [==============================] - 21s 54ms/step - loss: 0.3925 - acc: 0.8663 - val_loss: 0.5861 - val_acc: 0.8150
Epoch 29/50
390/390 [==============================] - 21s 55ms/step - loss: 0.3936 - acc: 0.8676 - val_loss: 0.5965 - val_acc: 0.8170
Epoch 30/50
390/390 [==============================] - 21s 55ms/step - loss: 0.3778 - acc: 0.8725 - val_loss: 0.5639 - val_acc: 0.8221
Epoch 31/50
390/390 [==============================] - 21s 54ms/step - loss: 0.3748 - acc: 0.8738 - val_loss: 0.5733 - val_acc: 0.8234
Epoch 32/50
390/390 [==============================] - 21s 53ms/step - loss: 0.3764 - acc: 0.8737 - val_loss: 0.6568 - val_acc: 0.8044
Epoch 33/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3660 - acc: 0.8766 - val_loss: 0.5935 - val_acc: 0.8171
Epoch 34/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3651 - acc: 0.8762 - val_loss: 0.5807 - val_acc: 0.8234
Epoch 35/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3651 - acc: 0.8764 - val_loss: 0.6006 - val_acc: 0.8187
Epoch 36/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3458 - acc: 0.8835 - val_loss: 0.5780 - val_acc: 0.8249
Epoch 37/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3474 - acc: 0.8817 - val_loss: 0.6285 - val_acc: 0.8107
Epoch 38/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3490 - acc: 0.8828 - val_loss: 0.5853 - val_acc: 0.8205
Epoch 39/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3458 - acc: 0.8837 - val_loss: 0.5685 - val_acc: 0.8229
Epoch 40/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3432 - acc: 0.8841 - val_loss: 0.5860 - val_acc: 0.8231
Epoch 41/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3337 - acc: 0.8887 - val_loss: 0.6081 - val_acc: 0.8184
Epoch 42/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3245 - acc: 0.8909 - val_loss: 0.6012 - val_acc: 0.8200
Epoch 43/50
390/390 [==============================] - 22s 56ms/step - loss: 0.3292 - acc: 0.8886 - val_loss: 0.5829 - val_acc: 0.8273
Epoch 44/50
390/390 [==============================] - 21s 54ms/step - loss: 0.3284 - acc: 0.8904 - val_loss: 0.5837 - val_acc: 0.8225
Epoch 45/50
390/390 [==============================] - 21s 53ms/step - loss: 0.3223 - acc: 0.8939 - val_loss: 0.5933 - val_acc: 0.8245
Epoch 46/50
390/390 [==============================] - 21s 53ms/step - loss: 0.3235 - acc: 0.8930 - val_loss: 0.5961 - val_acc: 0.8247
Epoch 47/50
390/390 [==============================] - 21s 54ms/step - loss: 0.3151 - acc: 0.8957 - val_loss: 0.5801 - val_acc: 0.8254
Epoch 48/50
390/390 [==============================] - 21s 55ms/step - loss: 0.3119 - acc: 0.8965 - val_loss: 0.6011 - val_acc: 0.8240
Epoch 49/50
390/390 [==============================] - 21s 55ms/step - loss: 0.3206 - acc: 0.8951 - val_loss: 0.5824 - val_acc: 0.8262
Epoch 50/50
390/390 [==============================] - 21s 55ms/step - loss: 0.3183 - acc: 0.8954 - val_loss: 0.5764 - val_acc: 0.8279
Model took 1045.25 seconds to train












---------------------------------------------------------------------------------------------------------------------------------------


Assignment2 :

The strategy followed based on the given 8 DNN codes:
1) No use of bias. Thus,added use_bias=False
2) The last convolution layer is sacrosanct. Thus removed Batchnormation() and Dropout() from it.
3) After training this network with batch_size=32 and epochs=20, have observed each epoch run took 35s to 36s and for the 20th epoch, 
   the acc=0.9995 , val_acc=0.9903  , which clearly states the problem of overfitting(OF).
   
Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 36s 600us/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0397 - val_acc: 0.9903
<keras.callbacks.History at 0x7f34b6a8c7f0>

Thus, introduced Dropout(0.1) after every convolution layer.

4) Dropout of 0.1 is added after every convolution layer.
5) Total params: 11,160
   Trainable params: 10,980
   Non-trainable params: 180
5) Score: [0.019890406434552095, 0.9943] This is achieved with batch_size=64,epochs=20

Log:
Train on 60000 samples, validate on 10000 samples
Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
60000/60000 [==============================] - 35s 582us/step - loss: 0.0219 - acc: 0.9929 - val_loss: 0.0277 - val_acc: 0.9922
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
60000/60000 [==============================] - 21s 356us/step - loss: 0.0184 - acc: 0.9938 - val_loss: 0.0213 - val_acc: 0.9928
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
60000/60000 [==============================] - 21s 355us/step - loss: 0.0173 - acc: 0.9940 - val_loss: 0.0236 - val_acc: 0.9930
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0149 - acc: 0.9950 - val_loss: 0.0199 - val_acc: 0.9938
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0156 - acc: 0.9945 - val_loss: 0.0210 - val_acc: 0.9940
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
60000/60000 [==============================] - 21s 356us/step - loss: 0.0144 - acc: 0.9950 - val_loss: 0.0213 - val_acc: 0.9931
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0133 - acc: 0.9955 - val_loss: 0.0191 - val_acc: 0.9942
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0133 - acc: 0.9955 - val_loss: 0.0200 - val_acc: 0.9939
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0128 - acc: 0.9957 - val_loss: 0.0207 - val_acc: 0.9937
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
60000/60000 [==============================] - 22s 365us/step - loss: 0.0129 - acc: 0.9956 - val_loss: 0.0210 - val_acc: 0.9934
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.0196 - val_acc: 0.9940
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0223 - val_acc: 0.9935
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.0205 - val_acc: 0.9939
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
60000/60000 [==============================] - 22s 361us/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.0198 - val_acc: 0.9938
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0108 - acc: 0.9961 - val_loss: 0.0206 - val_acc: 0.9938
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0113 - acc: 0.9957 - val_loss: 0.0207 - val_acc: 0.9941
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0116 - acc: 0.9960 - val_loss: 0.0193 - val_acc: 0.9943
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
60000/60000 [==============================] - 22s 359us/step - loss: 0.0109 - acc: 0.9963 - val_loss: 0.0198 - val_acc: 0.9941
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0118 - acc: 0.9961 - val_loss: 0.0200 - val_acc: 0.9941
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0199 - val_acc: 0.9943
<keras.callbacks.History at 0x7f34b53b60b8>


----------------------------------------------------------------------------------------------------------------------------------------




Assignment1 :
Print Score Result [0.17091783548227282, 0.9863]

1.Convolution- It is a method used to make the network learn and find out the edges,gradients,textures of the iput image. These are then combined to create the parts of objects and then an object.Convolution translates the input image through multiple layers.

2.Filters/Kernels - Channel is a container of a specific information.Kernel helps to extract this information.No. of kernels is equal to the no. of channels in the output. No. of channels in the input has no relation with the no. of channels in the output.

3.Epochs - Usually a dataset is comprised of multiple images. If the convolution completes one round of the dataset, it is called one epoch.

4.1X1 convolution - It is actually a single pixel.It helps in channel reduction by combining the features.It is used to reduce the number of channels and not to increase the no. of channels.

5.3X3 convolution - Using 3X3,we can have any kernel.The od number 3 is chosen to maintain the symmetry. Every single 3X3 is a superset of any 2X2.

6.Feature Maps-Thisis collection of all features.Kernel is not going to extract the feature but the feture itself should be present there so that it can be combined with other features.

7.Activation Function This is used in convolution method. Mostly commonly "relu" is used.

8.Receptive Field- Receptive field of the last layer must atleast be the size of the object and not the size of the full image.
The LRF local receptive field do not hold its neighbour's information.It is 3X3 in size.
The GRF Global receptive field  must see the whole object. It is 1X1 in size.




