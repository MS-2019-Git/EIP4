# EIP4
Assignment 3:

1) Base network validation accuracy and log:   Accuracy on test data is: 82.96

390/390 [==============================] - 18s 46ms/step - loss: 1.8517 - acc: 0.2901 - val_loss: 1.4679 - val_acc: 0.4633
Epoch 2/50
390/390 [==============================] - 9s 22ms/step - loss: 1.3247 - acc: 0.5172 - val_loss: 1.1024 - val_acc: 0.6037
Epoch 3/50
390/390 [==============================] - 9s 22ms/step - loss: 1.0996 - acc: 0.6100 - val_loss: 0.9347 - val_acc: 0.6694
Epoch 4/50
390/390 [==============================] - 9s 23ms/step - loss: 0.9537 - acc: 0.6674 - val_loss: 0.8596 - val_acc: 0.7069
Epoch 5/50
390/390 [==============================] - 9s 23ms/step - loss: 0.8469 - acc: 0.7078 - val_loss: 0.7689 - val_acc: 0.7354
Epoch 6/50
390/390 [==============================] - 9s 22ms/step - loss: 0.7804 - acc: 0.7326 - val_loss: 0.8390 - val_acc: 0.7219
Epoch 7/50
390/390 [==============================] - 9s 22ms/step - loss: 0.7358 - acc: 0.7479 - val_loss: 0.7439 - val_acc: 0.7486
Epoch 8/50
390/390 [==============================] - 9s 23ms/step - loss: 0.6798 - acc: 0.7683 - val_loss: 0.6456 - val_acc: 0.7831
Epoch 9/50
390/390 [==============================] - 9s 23ms/step - loss: 0.6465 - acc: 0.7807 - val_loss: 0.6349 - val_acc: 0.7882
Epoch 10/50
390/390 [==============================] - 9s 23ms/step - loss: 0.6122 - acc: 0.7928 - val_loss: 0.6049 - val_acc: 0.7915
Epoch 11/50
390/390 [==============================] - 9s 23ms/step - loss: 0.5841 - acc: 0.8003 - val_loss: 0.6133 - val_acc: 0.7991
Epoch 12/50
390/390 [==============================] - 9s 23ms/step - loss: 0.5615 - acc: 0.8100 - val_loss: 0.6483 - val_acc: 0.7881
Epoch 13/50
390/390 [==============================] - 9s 23ms/step - loss: 0.5412 - acc: 0.8160 - val_loss: 0.5872 - val_acc: 0.8088
Epoch 14/50
390/390 [==============================] - 9s 23ms/step - loss: 0.5284 - acc: 0.8206 - val_loss: 0.5781 - val_acc: 0.8119
Epoch 15/50
390/390 [==============================] - 9s 23ms/step - loss: 0.5141 - acc: 0.8233 - val_loss: 0.5643 - val_acc: 0.8146
Epoch 16/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4972 - acc: 0.8315 - val_loss: 0.5910 - val_acc: 0.8070
Epoch 17/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4851 - acc: 0.8347 - val_loss: 0.5682 - val_acc: 0.8126
Epoch 18/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4683 - acc: 0.8409 - val_loss: 0.5725 - val_acc: 0.8116
Epoch 19/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4594 - acc: 0.8437 - val_loss: 0.5776 - val_acc: 0.8172
Epoch 20/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4534 - acc: 0.8460 - val_loss: 0.5555 - val_acc: 0.8211
Epoch 21/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4370 - acc: 0.8512 - val_loss: 0.5673 - val_acc: 0.8139
Epoch 22/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4322 - acc: 0.8521 - val_loss: 0.5705 - val_acc: 0.8186
Epoch 23/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4209 - acc: 0.8582 - val_loss: 0.5858 - val_acc: 0.8125
Epoch 24/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4100 - acc: 0.8597 - val_loss: 0.5860 - val_acc: 0.8167
Epoch 25/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4098 - acc: 0.8606 - val_loss: 0.5774 - val_acc: 0.8118
Epoch 26/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4038 - acc: 0.8631 - val_loss: 0.5771 - val_acc: 0.8192
Epoch 27/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3891 - acc: 0.8683 - val_loss: 0.5672 - val_acc: 0.8195
Epoch 28/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3773 - acc: 0.8729 - val_loss: 0.6040 - val_acc: 0.8153
Epoch 29/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3759 - acc: 0.8736 - val_loss: 0.5726 - val_acc: 0.8175
Epoch 30/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3722 - acc: 0.8733 - val_loss: 0.5803 - val_acc: 0.8258
Epoch 31/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3699 - acc: 0.8750 - val_loss: 0.6202 - val_acc: 0.8080
Epoch 32/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3619 - acc: 0.8777 - val_loss: 0.6089 - val_acc: 0.8183
Epoch 33/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3630 - acc: 0.8771 - val_loss: 0.5925 - val_acc: 0.8241
Epoch 34/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3521 - acc: 0.8805 - val_loss: 0.5595 - val_acc: 0.8282
Epoch 35/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3479 - acc: 0.8836 - val_loss: 0.6015 - val_acc: 0.8213
Epoch 36/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3447 - acc: 0.8831 - val_loss: 0.6253 - val_acc: 0.8151
Epoch 37/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3475 - acc: 0.8834 - val_loss: 0.5826 - val_acc: 0.8262
Epoch 38/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3354 - acc: 0.8870 - val_loss: 0.5961 - val_acc: 0.8179
Epoch 39/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3397 - acc: 0.8856 - val_loss: 0.5959 - val_acc: 0.8277
Epoch 40/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3224 - acc: 0.8927 - val_loss: 0.5866 - val_acc: 0.8164
Epoch 41/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3310 - acc: 0.8902 - val_loss: 0.6073 - val_acc: 0.8217
Epoch 42/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3222 - acc: 0.8918 - val_loss: 0.5704 - val_acc: 0.8297
Epoch 43/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3261 - acc: 0.8914 - val_loss: 0.5928 - val_acc: 0.8213
Epoch 44/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3194 - acc: 0.8929 - val_loss: 0.6074 - val_acc: 0.8235
Epoch 45/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3256 - acc: 0.8912 - val_loss: 0.5831 - val_acc: 0.8266
Epoch 46/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3066 - acc: 0.8976 - val_loss: 0.6111 - val_acc: 0.8203
Epoch 47/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3014 - acc: 0.8977 - val_loss: 0.6060 - val_acc: 0.8222
Epoch 48/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3109 - acc: 0.8962 - val_loss: 0.5976 - val_acc: 0.8180
Epoch 49/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3131 - acc: 0.8963 - val_loss: 0.6161 - val_acc: 0.8235
Epoch 50/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3112 - acc: 0.8971 - val_loss: 0.5766 - val_acc: 0.8296
Model took 452.59 seconds to train




2) Strategy used for the network model of assignment 3
-------------------------------------------------------

 A)  To implement Depthwise Separable convolution used SeparableConv2D with no bias. This helped to reduce the no. of parameters to less      than 100,000. This works in 2 phases: convolve with 3X3X1 for each channel and then combines it using 1X1X#channelsX#filters.
     Thus the output image size remains equal to the input size but with increased no. of channels.
     
 B) Removed dense layers as they work as fully connected layers.
 
 C) Used Batch normalization to normalize the channels . As features and weight will be at similar scale.
 
 D) Used Activation function ReLu to attain the non-linearity.
 
 E) After the model to predict now i was able to see for few epochs , the val_acc is almost same or higher than the acc value.
   Therefore, dropout to a value of 0.1 is added to prevent this problem of overfitting by making the model learn from all of its inputs    instead of sample inputs.
   
 F) To increase the dataset to be trained, utilized the dataaugmentation code with horizonal flip with a shift of 50%.
 G) Calculated the output,receptive field and jump at each level.
 
 H) The learning rate used is 0.003 with ADAM algorithm.(When the model is trained  with 0.001 nad 0.005 as well but recievd 82.62% and     83.3%). Finally with 0.003 lr , the model got a test accuracy of 84.10 % with 128 batch_size and 50 epochs.
 
 I) The assignment3 model is trained with : Total params: 76,561
                           Trainable params: 75,197
                           Non-trainable params: 1,364
                           
 J) Achieved 84.10% at the 41th epoch.                          
 
 Calculation: For Input Image of 32X32X3	
 ----------------------------------------------
 
(k,p,s)|Input|Output|RF|Jump

(3,1,1)	 32	   32	  3X3 	1
(3,1,1)	 32	   32	  5X5	 1
(2,0,2)	 32	   16	  6X6	 2
(3,1,1)	 16	   16	 10X10	2
(3,1,1)	 16	   16	 14X14	2
(2,0,2)	 16	   8	  16X16	4
(3,1,1)	 8	    8	  24x24	4
(3,1,1)	 8	    8	  32x32	4
(2,0,2)	 8	    4	  36X36	8
(3,1,1)	 4	    4	  52x52	8
(3,0,1)	 4	    2	  68X68	8


Model Definition:
---------------------
# Define the model for assignment3
from keras.layers import SeparableConv2D
model = Sequential()

model.add(SeparableConv2D(32, 3, 3, border_mode='same', input_shape=(32, 32, 3))) # 32 , RF 3X3
model.add(Activation('relu'))

model.add(SeparableConv2D(32, 3, 3, border_mode='same',use_bias=False)) # 32 , RF 5X5
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(MaxPooling2D(pool_size=(2, 2))) #16,RF 6X6
model.add(Dropout(0.1))

model.add(SeparableConv2D(64, 3, 3, border_mode='same',use_bias=False)) #16,RF 10X10
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(SeparableConv2D(64, 3, 3, border_mode='same',use_bias=False))#16, RF 14x14
model.add(BatchNormalization())
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2, 2)))#8, RF 16x16
model.add(Dropout(0.1))

model.add(SeparableConv2D(128, 3, 3, border_mode='same',use_bias=False))#8, RF 24x24
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(SeparableConv2D(128, 3, 3, border_mode='same',use_bias=False))#8, RF 32x32
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(MaxPooling2D(pool_size=(2, 2)))#4, RF 36X36
model.add(Dropout(0.1))

model.add(SeparableConv2D(256, 3, 3, border_mode='same',use_bias=False))#4, RF 52x52
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(SeparableConv2D(10,3,3,border_mode='same',use_bias=False))#4 ,RF 68x68
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(SeparableConv2D(10, 4))
model.add(Flatten())
model.add(Activation('softmax'))

model.summary()

LOG of the model:
------------------------
Epoch 1/50

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
390/390 [==============================] - 53s 135ms/step - loss: 0.6209 - acc: 0.7904 - val_loss: 0.8235 - val_acc: 0.7379
Epoch 2/50

Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
390/390 [==============================] - 11s 28ms/step - loss: 0.5348 - acc: 0.8120 - val_loss: 0.8648 - val_acc: 0.7246
Epoch 3/50

Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
390/390 [==============================] - 11s 28ms/step - loss: 0.5035 - acc: 0.8221 - val_loss: 0.7032 - val_acc: 0.7709
Epoch 4/50

Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
390/390 [==============================] - 11s 28ms/step - loss: 0.4773 - acc: 0.8345 - val_loss: 0.6122 - val_acc: 0.7965
Epoch 5/50

Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
390/390 [==============================] - 11s 28ms/step - loss: 0.4602 - acc: 0.8388 - val_loss: 0.5512 - val_acc: 0.8188
Epoch 6/50

Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
390/390 [==============================] - 12s 30ms/step - loss: 0.4394 - acc: 0.8470 - val_loss: 0.5906 - val_acc: 0.8046
Epoch 7/50

Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
390/390 [==============================] - 13s 33ms/step - loss: 0.4236 - acc: 0.8523 - val_loss: 0.5983 - val_acc: 0.8026
Epoch 8/50

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
390/390 [==============================] - 12s 31ms/step - loss: 0.4098 - acc: 0.8539 - val_loss: 0.5489 - val_acc: 0.8219
Epoch 9/50

Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
390/390 [==============================] - 11s 28ms/step - loss: 0.3972 - acc: 0.8595 - val_loss: 0.5459 - val_acc: 0.8209
Epoch 10/50

Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
390/390 [==============================] - 11s 28ms/step - loss: 0.3870 - acc: 0.8632 - val_loss: 0.5104 - val_acc: 0.8316
Epoch 11/50

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
390/390 [==============================] - 12s 30ms/step - loss: 0.3756 - acc: 0.8672 - val_loss: 0.5320 - val_acc: 0.8300
Epoch 12/50

Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
390/390 [==============================] - 12s 31ms/step - loss: 0.3639 - acc: 0.8708 - val_loss: 0.5413 - val_acc: 0.8243
Epoch 13/50

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
390/390 [==============================] - 12s 31ms/step - loss: 0.3617 - acc: 0.8726 - val_loss: 0.5548 - val_acc: 0.8234
Epoch 14/50

Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
390/390 [==============================] - 12s 30ms/step - loss: 0.3523 - acc: 0.8739 - val_loss: 0.5394 - val_acc: 0.8307
Epoch 15/50

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
390/390 [==============================] - 11s 29ms/step - loss: 0.3435 - acc: 0.8773 - val_loss: 0.5342 - val_acc: 0.8299
Epoch 16/50

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
390/390 [==============================] - 11s 28ms/step - loss: 0.3398 - acc: 0.8795 - val_loss: 0.5385 - val_acc: 0.8305
Epoch 17/50

Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
390/390 [==============================] - 11s 29ms/step - loss: 0.3353 - acc: 0.8797 - val_loss: 0.5349 - val_acc: 0.8296
Epoch 18/50

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
390/390 [==============================] - 11s 28ms/step - loss: 0.3293 - acc: 0.8825 - val_loss: 0.5157 - val_acc: 0.8357
Epoch 19/50

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
390/390 [==============================] - 11s 28ms/step - loss: 0.3220 - acc: 0.8850 - val_loss: 0.5271 - val_acc: 0.8324
Epoch 20/50

Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
390/390 [==============================] - 11s 28ms/step - loss: 0.3219 - acc: 0.8869 - val_loss: 0.5303 - val_acc: 0.8329
Epoch 21/50

Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.
390/390 [==============================] - 11s 29ms/step - loss: 0.3158 - acc: 0.8864 - val_loss: 0.5238 - val_acc: 0.8352
Epoch 22/50

Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.
390/390 [==============================] - 11s 28ms/step - loss: 0.3106 - acc: 0.8891 - val_loss: 0.5107 - val_acc: 0.8396
Epoch 23/50

Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.
390/390 [==============================] - 11s 28ms/step - loss: 0.3110 - acc: 0.8900 - val_loss: 0.5276 - val_acc: 0.8332
Epoch 24/50

Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.
390/390 [==============================] - 11s 28ms/step - loss: 0.3043 - acc: 0.8905 - val_loss: 0.5225 - val_acc: 0.8360
Epoch 25/50

Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.
390/390 [==============================] - 11s 28ms/step - loss: 0.3019 - acc: 0.8917 - val_loss: 0.5284 - val_acc: 0.8355
Epoch 26/50

Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.
390/390 [==============================] - 11s 28ms/step - loss: 0.3000 - acc: 0.8931 - val_loss: 0.5181 - val_acc: 0.8371
Epoch 27/50

Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.
390/390 [==============================] - 11s 28ms/step - loss: 0.2929 - acc: 0.8939 - val_loss: 0.5287 - val_acc: 0.8351
Epoch 28/50

Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.
390/390 [==============================] - 11s 28ms/step - loss: 0.2940 - acc: 0.8950 - val_loss: 0.5219 - val_acc: 0.8354
Epoch 29/50

Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.
390/390 [==============================] - 11s 28ms/step - loss: 0.2875 - acc: 0.8964 - val_loss: 0.5350 - val_acc: 0.8330
Epoch 30/50

Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.
390/390 [==============================] - 11s 27ms/step - loss: 0.2889 - acc: 0.8970 - val_loss: 0.5323 - val_acc: 0.8384
Epoch 31/50

Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.
390/390 [==============================] - 11s 28ms/step - loss: 0.2834 - acc: 0.8984 - val_loss: 0.5361 - val_acc: 0.8367
Epoch 32/50

Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.
390/390 [==============================] - 11s 28ms/step - loss: 0.2825 - acc: 0.8984 - val_loss: 0.5485 - val_acc: 0.8297
Epoch 33/50

Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.
390/390 [==============================] - 11s 28ms/step - loss: 0.2839 - acc: 0.8979 - val_loss: 0.5332 - val_acc: 0.8382
Epoch 34/50

Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.
390/390 [==============================] - 11s 28ms/step - loss: 0.2797 - acc: 0.9005 - val_loss: 0.5303 - val_acc: 0.8406
Epoch 35/50

Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.
390/390 [==============================] - 11s 29ms/step - loss: 0.2772 - acc: 0.9008 - val_loss: 0.5468 - val_acc: 0.8339
Epoch 36/50

Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.
390/390 [==============================] - 11s 28ms/step - loss: 0.2812 - acc: 0.8992 - val_loss: 0.5297 - val_acc: 0.8394
Epoch 37/50

Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.
390/390 [==============================] - 11s 28ms/step - loss: 0.2687 - acc: 0.9039 - val_loss: 0.5375 - val_acc: 0.8356
Epoch 38/50

Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.
390/390 [==============================] - 11s 28ms/step - loss: 0.2747 - acc: 0.9012 - val_loss: 0.5337 - val_acc: 0.8358
Epoch 39/50

Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.
390/390 [==============================] - 11s 28ms/step - loss: 0.2694 - acc: 0.9045 - val_loss: 0.5257 - val_acc: 0.8394
Epoch 40/50

Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.
390/390 [==============================] - 11s 28ms/step - loss: 0.2684 - acc: 0.9042 - val_loss: 0.5289 - val_acc: 0.8410
Epoch 41/50

Epoch 00041: LearningRateScheduler setting learning rate to 0.0002180233.
390/390 [==============================] - 11s 28ms/step - loss: 0.2672 - acc: 0.9018 - val_loss: 0.5283 - val_acc: 0.8406
Epoch 42/50

Epoch 00042: LearningRateScheduler setting learning rate to 0.0002130833.
390/390 [==============================] - 11s 28ms/step - loss: 0.2626 - acc: 0.9048 - val_loss: 0.5390 - val_acc: 0.8376
Epoch 43/50

Epoch 00043: LearningRateScheduler setting learning rate to 0.0002083623.
390/390 [==============================] - 11s 28ms/step - loss: 0.2648 - acc: 0.9054 - val_loss: 0.5350 - val_acc: 0.8363
Epoch 44/50

Epoch 00044: LearningRateScheduler setting learning rate to 0.0002038459.
390/390 [==============================] - 11s 28ms/step - loss: 0.2634 - acc: 0.9043 - val_loss: 0.5285 - val_acc: 0.8391
Epoch 45/50

Epoch 00045: LearningRateScheduler setting learning rate to 0.0001995211.
390/390 [==============================] - 11s 28ms/step - loss: 0.2606 - acc: 0.9069 - val_loss: 0.5267 - val_acc: 0.8401
Epoch 46/50

Epoch 00046: LearningRateScheduler setting learning rate to 0.0001953761.
390/390 [==============================] - 11s 27ms/step - loss: 0.2622 - acc: 0.9044 - val_loss: 0.5305 - val_acc: 0.8400
Epoch 47/50

Epoch 00047: LearningRateScheduler setting learning rate to 0.0001913998.
390/390 [==============================] - 11s 28ms/step - loss: 0.2578 - acc: 0.9072 - val_loss: 0.5312 - val_acc: 0.8407
Epoch 48/50

Epoch 00048: LearningRateScheduler setting learning rate to 0.0001875821.
390/390 [==============================] - 11s 28ms/step - loss: 0.2556 - acc: 0.9088 - val_loss: 0.5322 - val_acc: 0.8405
Epoch 49/50

Epoch 00049: LearningRateScheduler setting learning rate to 0.0001839137.
390/390 [==============================] - 11s 28ms/step - loss: 0.2589 - acc: 0.9071 - val_loss: 0.5354 - val_acc: 0.8387
Epoch 50/50

Epoch 00050: LearningRateScheduler setting learning rate to 0.000180386.
390/390 [==============================] - 11s 28ms/step - loss: 0.2521 - acc: 0.9088 - val_loss: 0.5429 - val_acc: 0.8373
Model took 595.91 seconds to train

Accuracy on test data is: 83.73









---------------------------------------------------------------------------------------------------------------------------------------


Assignment2 :

The strategy followed based on the given 8 DNN codes:
1) No use of bias. Thus,added use_bias=False
2) The last convolution layer is sacrosanct. Thus removed Batchnormation() and Dropout() from it.
3) After training this network with batch_size=32 and epochs=20, have observed each epoch run took 35s to 36s and for the 20th epoch, 
   the acc=0.9995 , val_acc=0.9903  , which clearly states the problem of overfitting(OF).
   
Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 36s 600us/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0397 - val_acc: 0.9903
<keras.callbacks.History at 0x7f34b6a8c7f0>

Thus, introduced Dropout(0.1) after every convolution layer.

4) Dropout of 0.1 is added after every convolution layer.
5) Total params: 11,160
   Trainable params: 10,980
   Non-trainable params: 180
5) Score: [0.019890406434552095, 0.9943] This is achieved with batch_size=64,epochs=20

Log:
Train on 60000 samples, validate on 10000 samples
Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
60000/60000 [==============================] - 35s 582us/step - loss: 0.0219 - acc: 0.9929 - val_loss: 0.0277 - val_acc: 0.9922
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
60000/60000 [==============================] - 21s 356us/step - loss: 0.0184 - acc: 0.9938 - val_loss: 0.0213 - val_acc: 0.9928
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
60000/60000 [==============================] - 21s 355us/step - loss: 0.0173 - acc: 0.9940 - val_loss: 0.0236 - val_acc: 0.9930
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0149 - acc: 0.9950 - val_loss: 0.0199 - val_acc: 0.9938
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0156 - acc: 0.9945 - val_loss: 0.0210 - val_acc: 0.9940
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
60000/60000 [==============================] - 21s 356us/step - loss: 0.0144 - acc: 0.9950 - val_loss: 0.0213 - val_acc: 0.9931
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0133 - acc: 0.9955 - val_loss: 0.0191 - val_acc: 0.9942
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0133 - acc: 0.9955 - val_loss: 0.0200 - val_acc: 0.9939
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0128 - acc: 0.9957 - val_loss: 0.0207 - val_acc: 0.9937
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
60000/60000 [==============================] - 22s 365us/step - loss: 0.0129 - acc: 0.9956 - val_loss: 0.0210 - val_acc: 0.9934
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.0196 - val_acc: 0.9940
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0223 - val_acc: 0.9935
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.0205 - val_acc: 0.9939
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
60000/60000 [==============================] - 22s 361us/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.0198 - val_acc: 0.9938
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0108 - acc: 0.9961 - val_loss: 0.0206 - val_acc: 0.9938
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0113 - acc: 0.9957 - val_loss: 0.0207 - val_acc: 0.9941
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0116 - acc: 0.9960 - val_loss: 0.0193 - val_acc: 0.9943
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
60000/60000 [==============================] - 22s 359us/step - loss: 0.0109 - acc: 0.9963 - val_loss: 0.0198 - val_acc: 0.9941
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0118 - acc: 0.9961 - val_loss: 0.0200 - val_acc: 0.9941
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0199 - val_acc: 0.9943
<keras.callbacks.History at 0x7f34b53b60b8>


----------------------------------------------------------------------------------------------------------------------------------------




Assignment1 :
Print Score Result [0.17091783548227282, 0.9863]

1.Convolution- It is a method used to make the network learn and find out the edges,gradients,textures of the iput image. These are then combined to create the parts of objects and then an object.Convolution translates the input image through multiple layers.

2.Filters/Kernels - Channel is a container of a specific information.Kernel helps to extract this information.No. of kernels is equal to the no. of channels in the output. No. of channels in the input has no relation with the no. of channels in the output.

3.Epochs - Usually a dataset is comprised of multiple images. If the convolution completes one round of the dataset, it is called one epoch.

4.1X1 convolution - It is actually a single pixel.It helps in channel reduction by combining the features.It is used to reduce the number of channels and not to increase the no. of channels.

5.3X3 convolution - Using 3X3,we can have any kernel.The od number 3 is chosen to maintain the symmetry. Every single 3X3 is a superset of any 2X2.

6.Feature Maps-Thisis collection of all features.Kernel is not going to extract the feature but the feture itself should be present there so that it can be combined with other features.

7.Activation Function This is used in convolution method. Mostly commonly "relu" is used.

8.Receptive Field- Receptive field of the last layer must atleast be the size of the object and not the size of the full image.
The LRF local receptive field do not hold its neighbour's information.It is 3X3 in size.
The GRF Global receptive field  must see the whole object. It is 1X1 in size.




