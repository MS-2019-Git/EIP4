# EIP4
Assignment 3:

1) Base network validation accuracy and log:   Accuracy on test data is: 82.96

390/390 [==============================] - 18s 46ms/step - loss: 1.8517 - acc: 0.2901 - val_loss: 1.4679 - val_acc: 0.4633
Epoch 2/50
390/390 [==============================] - 9s 22ms/step - loss: 1.3247 - acc: 0.5172 - val_loss: 1.1024 - val_acc: 0.6037
Epoch 3/50
390/390 [==============================] - 9s 22ms/step - loss: 1.0996 - acc: 0.6100 - val_loss: 0.9347 - val_acc: 0.6694
Epoch 4/50
390/390 [==============================] - 9s 23ms/step - loss: 0.9537 - acc: 0.6674 - val_loss: 0.8596 - val_acc: 0.7069
Epoch 5/50
390/390 [==============================] - 9s 23ms/step - loss: 0.8469 - acc: 0.7078 - val_loss: 0.7689 - val_acc: 0.7354
Epoch 6/50
390/390 [==============================] - 9s 22ms/step - loss: 0.7804 - acc: 0.7326 - val_loss: 0.8390 - val_acc: 0.7219
Epoch 7/50
390/390 [==============================] - 9s 22ms/step - loss: 0.7358 - acc: 0.7479 - val_loss: 0.7439 - val_acc: 0.7486
Epoch 8/50
390/390 [==============================] - 9s 23ms/step - loss: 0.6798 - acc: 0.7683 - val_loss: 0.6456 - val_acc: 0.7831
Epoch 9/50
390/390 [==============================] - 9s 23ms/step - loss: 0.6465 - acc: 0.7807 - val_loss: 0.6349 - val_acc: 0.7882
Epoch 10/50
390/390 [==============================] - 9s 23ms/step - loss: 0.6122 - acc: 0.7928 - val_loss: 0.6049 - val_acc: 0.7915
Epoch 11/50
390/390 [==============================] - 9s 23ms/step - loss: 0.5841 - acc: 0.8003 - val_loss: 0.6133 - val_acc: 0.7991
Epoch 12/50
390/390 [==============================] - 9s 23ms/step - loss: 0.5615 - acc: 0.8100 - val_loss: 0.6483 - val_acc: 0.7881
Epoch 13/50
390/390 [==============================] - 9s 23ms/step - loss: 0.5412 - acc: 0.8160 - val_loss: 0.5872 - val_acc: 0.8088
Epoch 14/50
390/390 [==============================] - 9s 23ms/step - loss: 0.5284 - acc: 0.8206 - val_loss: 0.5781 - val_acc: 0.8119
Epoch 15/50
390/390 [==============================] - 9s 23ms/step - loss: 0.5141 - acc: 0.8233 - val_loss: 0.5643 - val_acc: 0.8146
Epoch 16/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4972 - acc: 0.8315 - val_loss: 0.5910 - val_acc: 0.8070
Epoch 17/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4851 - acc: 0.8347 - val_loss: 0.5682 - val_acc: 0.8126
Epoch 18/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4683 - acc: 0.8409 - val_loss: 0.5725 - val_acc: 0.8116
Epoch 19/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4594 - acc: 0.8437 - val_loss: 0.5776 - val_acc: 0.8172
Epoch 20/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4534 - acc: 0.8460 - val_loss: 0.5555 - val_acc: 0.8211
Epoch 21/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4370 - acc: 0.8512 - val_loss: 0.5673 - val_acc: 0.8139
Epoch 22/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4322 - acc: 0.8521 - val_loss: 0.5705 - val_acc: 0.8186
Epoch 23/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4209 - acc: 0.8582 - val_loss: 0.5858 - val_acc: 0.8125
Epoch 24/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4100 - acc: 0.8597 - val_loss: 0.5860 - val_acc: 0.8167
Epoch 25/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4098 - acc: 0.8606 - val_loss: 0.5774 - val_acc: 0.8118
Epoch 26/50
390/390 [==============================] - 9s 23ms/step - loss: 0.4038 - acc: 0.8631 - val_loss: 0.5771 - val_acc: 0.8192
Epoch 27/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3891 - acc: 0.8683 - val_loss: 0.5672 - val_acc: 0.8195
Epoch 28/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3773 - acc: 0.8729 - val_loss: 0.6040 - val_acc: 0.8153
Epoch 29/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3759 - acc: 0.8736 - val_loss: 0.5726 - val_acc: 0.8175
Epoch 30/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3722 - acc: 0.8733 - val_loss: 0.5803 - val_acc: 0.8258
Epoch 31/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3699 - acc: 0.8750 - val_loss: 0.6202 - val_acc: 0.8080
Epoch 32/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3619 - acc: 0.8777 - val_loss: 0.6089 - val_acc: 0.8183
Epoch 33/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3630 - acc: 0.8771 - val_loss: 0.5925 - val_acc: 0.8241
Epoch 34/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3521 - acc: 0.8805 - val_loss: 0.5595 - val_acc: 0.8282
Epoch 35/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3479 - acc: 0.8836 - val_loss: 0.6015 - val_acc: 0.8213
Epoch 36/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3447 - acc: 0.8831 - val_loss: 0.6253 - val_acc: 0.8151
Epoch 37/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3475 - acc: 0.8834 - val_loss: 0.5826 - val_acc: 0.8262
Epoch 38/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3354 - acc: 0.8870 - val_loss: 0.5961 - val_acc: 0.8179
Epoch 39/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3397 - acc: 0.8856 - val_loss: 0.5959 - val_acc: 0.8277
Epoch 40/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3224 - acc: 0.8927 - val_loss: 0.5866 - val_acc: 0.8164
Epoch 41/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3310 - acc: 0.8902 - val_loss: 0.6073 - val_acc: 0.8217
Epoch 42/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3222 - acc: 0.8918 - val_loss: 0.5704 - val_acc: 0.8297
Epoch 43/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3261 - acc: 0.8914 - val_loss: 0.5928 - val_acc: 0.8213
Epoch 44/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3194 - acc: 0.8929 - val_loss: 0.6074 - val_acc: 0.8235
Epoch 45/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3256 - acc: 0.8912 - val_loss: 0.5831 - val_acc: 0.8266
Epoch 46/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3066 - acc: 0.8976 - val_loss: 0.6111 - val_acc: 0.8203
Epoch 47/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3014 - acc: 0.8977 - val_loss: 0.6060 - val_acc: 0.8222
Epoch 48/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3109 - acc: 0.8962 - val_loss: 0.5976 - val_acc: 0.8180
Epoch 49/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3131 - acc: 0.8963 - val_loss: 0.6161 - val_acc: 0.8235
Epoch 50/50
390/390 [==============================] - 9s 23ms/step - loss: 0.3112 - acc: 0.8971 - val_loss: 0.5766 - val_acc: 0.8296
Model took 452.59 seconds to train




2) Strategy used for the network model of assignment 3:

 A)  To implement Depthwise Separable convolution used SeparableConv2D with no bias. This helped to reduce the no. of parameters to less      than 100,000. This works in 2 phases: convolve with 3X3X1 for each channel and then combines it using 1X1X#channelsX#filters.
     Thus the output image size remains equal to the input size but with increased no. of channels.
     
 B) Removed dense layers as they work as fully connected layers.
 
 C) Used Batch normalization to normalize the channels . As features and weight will be at similar scale.
 
 D) Used Activation function ReLu to attain the non-linearity.
 
 E) After the model to predict now i was able to see for few epochs , the val_acc is almost same or higher than the acc value.
 Epoch 50/50
390/390 [==============================] - 16s 41ms/step - loss: 0.2710 - acc: 0.9040 - val_loss: 0.5443 - val_acc: 0.8348

   Therefore, dropout to a value of 0.1 is added to prevent this problem of overfitting by making the model learn from all of its inputs    instead of sample inputs.
   
 F) To increase the dataset to be trained, utilized the dataaugmentation code with horizonal flip with a shift of 50%.
 G) Calculated the output,receptive field and jump at each level.
 
 H) The learning rate used is 0.003 with ADAM algorithm.(When the model is trained  with 0.001 nad 0.005 as well but recievd 82.62% and     83.3%). Finally with 0.003 lr , the model got a test accuracy of % with 128 batch_size and 50 epochs.
 
 I) The assignment3 model is trained with : Total params: 76,561
                           Trainable params: 75,197
                           Non-trainable params: 1,364
 

Model Definition:
---------------------
# Define the model for assignment3
from keras.layers import SeparableConv2D
model = Sequential()

model.add(SeparableConv2D(32, 3, 3, border_mode='same', input_shape=(32, 32, 3))) # 32 , RF 3X3
model.add(Activation('relu'))

model.add(SeparableConv2D(32, 3, 3, border_mode='same',use_bias=False)) # 32 , RF 5X5
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(MaxPooling2D(pool_size=(2, 2))) #16,RF 6X6

model.add(SeparableConv2D(64, 3, 3, border_mode='same',use_bias=False)) #16,RF 10X10
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(SeparableConv2D(64, 3, 3, border_mode='same',use_bias=False))#16, RF 14x14
model.add(BatchNormalization())
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2, 2)))#8, RF 16x16

model.add(SeparableConv2D(128, 3, 3, border_mode='same',use_bias=False))#8, RF 24x24
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(SeparableConv2D(128, 3, 3, border_mode='same',use_bias=False))#8, RF 32x32
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(MaxPooling2D(pool_size=(2, 2)))#4, RF 36X36

model.add(SeparableConv2D(256, 3, 3, border_mode='same',use_bias=False))#4, RF 52x52
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(SeparableConv2D(10,3,3,border_mode='same',use_bias=False))#4 ,RF 68x68
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(SeparableConv2D(10, 4))#2, RF 84X84
model.add(Flatten())
model.add(Activation('softmax'))

model.summary()









---------------------------------------------------------------------------------------------------------------------------------------


Assignment2 :

The strategy followed based on the given 8 DNN codes:
1) No use of bias. Thus,added use_bias=False
2) The last convolution layer is sacrosanct. Thus removed Batchnormation() and Dropout() from it.
3) After training this network with batch_size=32 and epochs=20, have observed each epoch run took 35s to 36s and for the 20th epoch, 
   the acc=0.9995 , val_acc=0.9903  , which clearly states the problem of overfitting(OF).
   
Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 36s 600us/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0397 - val_acc: 0.9903
<keras.callbacks.History at 0x7f34b6a8c7f0>

Thus, introduced Dropout(0.1) after every convolution layer.

4) Dropout of 0.1 is added after every convolution layer.
5) Total params: 11,160
   Trainable params: 10,980
   Non-trainable params: 180
5) Score: [0.019890406434552095, 0.9943] This is achieved with batch_size=64,epochs=20

Log:
Train on 60000 samples, validate on 10000 samples
Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
60000/60000 [==============================] - 35s 582us/step - loss: 0.0219 - acc: 0.9929 - val_loss: 0.0277 - val_acc: 0.9922
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
60000/60000 [==============================] - 21s 356us/step - loss: 0.0184 - acc: 0.9938 - val_loss: 0.0213 - val_acc: 0.9928
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
60000/60000 [==============================] - 21s 355us/step - loss: 0.0173 - acc: 0.9940 - val_loss: 0.0236 - val_acc: 0.9930
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0149 - acc: 0.9950 - val_loss: 0.0199 - val_acc: 0.9938
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0156 - acc: 0.9945 - val_loss: 0.0210 - val_acc: 0.9940
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
60000/60000 [==============================] - 21s 356us/step - loss: 0.0144 - acc: 0.9950 - val_loss: 0.0213 - val_acc: 0.9931
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0133 - acc: 0.9955 - val_loss: 0.0191 - val_acc: 0.9942
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0133 - acc: 0.9955 - val_loss: 0.0200 - val_acc: 0.9939
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0128 - acc: 0.9957 - val_loss: 0.0207 - val_acc: 0.9937
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
60000/60000 [==============================] - 22s 365us/step - loss: 0.0129 - acc: 0.9956 - val_loss: 0.0210 - val_acc: 0.9934
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.0196 - val_acc: 0.9940
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0223 - val_acc: 0.9935
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.0205 - val_acc: 0.9939
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
60000/60000 [==============================] - 22s 361us/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.0198 - val_acc: 0.9938
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
60000/60000 [==============================] - 22s 360us/step - loss: 0.0108 - acc: 0.9961 - val_loss: 0.0206 - val_acc: 0.9938
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0113 - acc: 0.9957 - val_loss: 0.0207 - val_acc: 0.9941
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0116 - acc: 0.9960 - val_loss: 0.0193 - val_acc: 0.9943
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
60000/60000 [==============================] - 22s 359us/step - loss: 0.0109 - acc: 0.9963 - val_loss: 0.0198 - val_acc: 0.9941
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
60000/60000 [==============================] - 21s 358us/step - loss: 0.0118 - acc: 0.9961 - val_loss: 0.0200 - val_acc: 0.9941
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 21s 357us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0199 - val_acc: 0.9943
<keras.callbacks.History at 0x7f34b53b60b8>


----------------------------------------------------------------------------------------------------------------------------------------




Assignment1 :
Print Score Result [0.17091783548227282, 0.9863]

1.Convolution- It is a method used to make the network learn and find out the edges,gradients,textures of the iput image. These are then combined to create the parts of objects and then an object.Convolution translates the input image through multiple layers.

2.Filters/Kernels - Channel is a container of a specific information.Kernel helps to extract this information.No. of kernels is equal to the no. of channels in the output. No. of channels in the input has no relation with the no. of channels in the output.

3.Epochs - Usually a dataset is comprised of multiple images. If the convolution completes one round of the dataset, it is called one epoch.

4.1X1 convolution - It is actually a single pixel.It helps in channel reduction by combining the features.It is used to reduce the number of channels and not to increase the no. of channels.

5.3X3 convolution - Using 3X3,we can have any kernel.The od number 3 is chosen to maintain the symmetry. Every single 3X3 is a superset of any 2X2.

6.Feature Maps-Thisis collection of all features.Kernel is not going to extract the feature but the feture itself should be present there so that it can be combined with other features.

7.Activation Function This is used in convolution method. Mostly commonly "relu" is used.

8.Receptive Field- Receptive field of the last layer must atleast be the size of the object and not the size of the full image.
The LRF local receptive field do not hold its neighbour's information.It is 3X3 in size.
The GRF Global receptive field  must see the whole object. It is 1X1 in size.




