{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Assignment5_PersonAttributes_run1_ResNet50.ipynb","provenance":[{"file_id":"1gZB7T7Giks2MyaT9HHefRMJEJR8inh0M","timestamp":1576840897827},{"file_id":"1QyoJ3U4SUZjRYLPbKeody0UHy1iNqdh5","timestamp":1576247007422}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Gyq8CE4ug5BK","colab_type":"code","colab":{}},"source":["# mount gdrive and unzip data\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n","# look for `hvc_annotations.csv` file and `resized` dir\n","%ls "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kErEkfnQn-U","colab_type":"code","colab":{}},"source":["# example of loading an image with the Keras API\n","from keras.preprocessing.image import load_img\n","# load the image\n","img = load_img('resized/16.jpg')\n","# report details about the image\n","print(type(img))\n","print(img.format)\n","print(img.mode)\n","print(img.size)\n","# show the image\n","img.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bYbNQzK6kj94","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","\n","import cv2\n","import json\n","\n","import numpy as np\n","import pandas as pd\n","\n","from functools import partial\n","from pathlib import Path \n","from tqdm import tqdm\n","\n","from google.colab.patches import cv2_imshow\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","\n","\n","from keras.applications import ResNet50\n","from keras.layers.core import Dropout\n","from keras.layers.core import Flatten\n","from keras.layers.core import Dense\n","from keras.layers import Input\n","from keras.models import Model\n","from keras.optimizers import SGD\n","from keras.preprocessing.image import ImageDataGenerator\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"202OJva345WA","colab_type":"code","colab":{}},"source":["# load annotations\n","\n","from keras.preprocessing import image \n","df = pd.read_csv(\"hvc_annotations.csv\")\n","del df[\"filename\"] # remove unwanted column\n","print(df.shape)\n","#df.head()\n","#df.tail()\n","\n","\n","# one hot encoding of labels\n","one_hot_df = pd.concat([\n","    df[[\"image_path\"]],\n","    pd.get_dummies(df.gender, prefix=\"gender\"),\n","    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n","    pd.get_dummies(df.age, prefix=\"age\"),\n","    pd.get_dummies(df.weight, prefix=\"weight\"),\n","    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n","    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n","    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n","    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n","], axis = 1)\n","\n","print(one_hot_df.shape)\n","one_hot_df.head().T"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ll94zTv6w5i","colab_type":"code","colab":{}},"source":["import keras\n","import numpy as np\n","\n","# Label columns per attribute\n","_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n","_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n","_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n","_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n","_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n","_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n","_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n","_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n","\n","class PersonDataGenerator(keras.utils.Sequence):\n","    \"\"\"Ground truth data generator\"\"\"\n","\n","    \n","    def __init__(self, df, batch_size=32, shuffle=True):\n","        self.df = df\n","        self.batch_size=batch_size\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        return int(np.floor(self.df.shape[0] / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        \"\"\"fetch batched images and targets\"\"\"\n","        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n","        items = self.df.iloc[batch_slice]\n","        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n","        target = {\n","            \"gender_output\": items[_gender_cols_].values,\n","            \"image_quality_output\": items[_imagequality_cols_].values,\n","            \"age_output\": items[_age_cols_].values,\n","            \"weight_output\": items[_weight_cols_].values,\n","            \"bag_output\": items[_carryingbag_cols_].values,\n","            \"pose_output\": items[_bodypose_cols_].values,\n","            \"footwear_output\": items[_footwear_cols_].values,\n","            \"emotion_output\": items[_emotion_cols_].values,\n","        }\n","        return image, target\n","\n","    def on_epoch_end(self):\n","        \"\"\"Updates indexes after each epoch\"\"\"\n","        if self.shuffle == True:\n","            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTiOi5tVBnhS","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n","train_df.shape, val_df.shape\n","train_df.head()\n","\n","# create train and validation data generators\n","train_gen = PersonDataGenerator(train_df, batch_size=32)\n","valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)\n","\n","# get number of output units from data\n","images, targets = next(iter(train_gen))\n","num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n","num_units"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"03W8Pagg_Ppp","colab_type":"code","colab":{}},"source":["from keras.layers import AveragePooling2D\n","\n","backbone = ResNet50(\n","    weights=None,\n","    include_top=False, \n","    input_tensor=Input(shape=(224, 224, 3))\n",")\n","\n","\n","neck = backbone.output\n","neck = AveragePooling2D()(neck)\n","neck = Flatten(name=\"flatten\")(neck)\n","\n","\n","def build_tower(in_layer):\n","    #neck = Dropout(0.5)(in_layer)\n","    #neck = Flatten(name=\"flatten\")(in_layer)\n","    neck = Dense(128, activation=\"relu\")(in_layer)\n","    #neck = Dropout(0.5)(in_layer)\n","    #neck = Dense(128, activation=\"relu\")(neck)\n","    return neck\n","\n","\n","def build_head(name, in_layer):\n","    return Dense(\n","        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n","    )(in_layer)\n","\n","# heads\n","gender = build_head(\"gender\", build_tower(neck))\n","image_quality = build_head(\"image_quality\", build_tower(neck))\n","age = build_head(\"age\", build_tower(neck))\n","weight = build_head(\"weight\", build_tower(neck))\n","bag = build_head(\"bag\", build_tower(neck))\n","footwear = build_head(\"footwear\", build_tower(neck))\n","emotion = build_head(\"emotion\", build_tower(neck))\n","pose = build_head(\"pose\", build_tower(neck))\n","\n","\n","model = Model(\n","    inputs=backbone.input, \n","    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n","   )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFgKjMCEbYMK","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"RfPG9C2eA1zn","colab_type":"code","colab":{}},"source":["# losses = {\n","# \t\"gender_output\": \"binary_crossentropy\",\n","# \t\"image_quality_output\": \"categorical_crossentropy\",\n","# \t\"age_output\": \"categorical_crossentropy\",\n","# \t\"weight_output\": \"categorical_crossentropy\",\n","\n","# }\n","# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n","from keras.optimizers import Adam\n","opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n","model.compile(\n","    optimizer=opt,\n","    loss=\"categorical_crossentropy\", \n","    # loss_weights=loss_weights, \n","    metrics=[\"accuracy\"]\n",")\n","\n","# # freeze backbone\n","# for layer in backbone.layers:\n","# \tlayer.trainable = False\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rz2qzr_TpA0a","colab_type":"code","colab":{}},"source":["def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 0.001 \n","    if epoch > 50:\n","        lr *= 0.1   \n","    if epoch > 40:\n","        lr *= 0.1\n","    elif epoch > 30:\n","        lr *= 0.1\n","    print('Learning rate: ', lr)\n","    return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FY9hkZ7giOMJ","colab_type":"code","colab":{}},"source":["# Prepare model model saving directory.\n","import os\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler,ReduceLROnPlateau\n","save_dir = os.path.join(os.getcwd(), 'saved_models')\n","model_name = 'cifar10_%s_model.{epoch:03d}.h5' \n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","def scheduler(epoch, lr):  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n","\n","lr_scheduler = LearningRateScheduler(scheduler, verbose=1)\n","#lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpxv41EyNmN4","colab_type":"code","colab":{}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","import time\n","datagen = ImageDataGenerator(zoom_range=0.0, \n","                             horizontal_flip=False)\n","\n","\n","# Train the model\n","start = time.time()\n","model_info = model.fit_generator(generator=train_gen,\n","                                 epochs = 5,\n","                                 use_multiprocessing=True,\n","                                 workers=1,\n","                                 validation_data = valid_gen,\n","                                 verbose=1,callbacks=callbacks)\n","end = time.time()\n","print (\"Model took %0.2f seconds to train\"%(end - start))\n","\n","scores=model.evaluate_generator(valid_gen,verbose=1)\n","model.metrics_names\n","dict(zip(model.metrics_names,scores))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrRIjnK3uwR9","colab_type":"code","colab":{}},"source":["def plot_model_history(model_history):\n","    fig, axs = plt.subplots(1,2,figsize=(15,5))\n","    # summarize history for accuracy\n","    axs[0].plot(range(1,len(model_history.history['gender_output_acc'])+1),model_history.history['gender_output_acc'])\n","    axs[0].plot(range(1,len(model_history.history['val_gender_output_acc'])+1),model_history.history['val_gender_output_acc'])\n","    axs[0].set_title('Model Accuracy')\n","    axs[0].set_ylabel('Accuracy')\n","    axs[0].set_xlabel('Epoch')\n","    axs[0].set_xticks(np.arange(1,len(model_history.history['gender_output_acc'])+1),len(model_history.history['gender_output_acc'])/10)\n","    axs[0].legend(['train', 'val'], loc='best')\n","    # summarize history for loss\n","    axs[1].plot(range(1,len(model_history.history['gender_output_loss'])+1),model_history.history['gender_output_loss'])\n","    axs[1].plot(range(1,len(model_history.history['val_gender_output_loss'])+1),model_history.history['val_gender_output_loss'])\n","    axs[1].set_title('Model Loss')\n","    axs[1].set_ylabel('Loss')\n","    axs[1].set_xlabel('Epoch')\n","    axs[1].set_xticks(np.arange(1,len(model_history.history['gender_output_loss'])+1),len(model_history.history['gender_output_loss'])/10)\n","    axs[1].legend(['train', 'val'], loc='best')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kg39orEcu718","colab_type":"code","colab":{}},"source":["# plot model history\n","import matplotlib.pyplot as plt\n","plot_model_history(model_info)\n","# compute test accuracy\n","print (\"Accuracy on test data is: %0.2f\"%accuracy(valid_gen, model))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zI1hJb4qM6OH","colab_type":"code","colab":{}},"source":["from keras.utils.vis_utils import plot_model\n","plot_model(model,to_file='model_plot.png',show_shapes=True, show_layer_names=True)"],"execution_count":0,"outputs":[]}]}